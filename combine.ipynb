{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXP8C6wA4l/iXx4fU7JGEa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiyuan-95/Amex-default-prediction-/blob/main/combine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYytYuR4ql3f",
        "outputId": "f0634e6b-5c1f-41dc-e10f-cba8b1ba5ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=t3sIXw1LRkUHLlHka70VUEWNqdgLaf&prompt=consent&access_type=offline&code_challenge=xtmWTy6bc5Ab5n1153R3pp8vX816Q4JaFhbtI1418TQ&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AZEOvhWiWI5vvMmLEgZkFBpQ3YsSXUn3vGPa3KZizmBG7aJ5il6HWV8QOuFnqGM_M9uc1g\n",
            "\n",
            "You are now logged in as [zhiyuan.jin1201@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters create combine --enable-component-gateway --region us-west1 --zone us-west1-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 100 --image-version 2.0-debian10 --project bda-12345"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Ch8Y1LKlNA",
        "outputId": "6f49dc31-969e-43c8-de80-b50770b38cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on operation [projects/bda-12345/regions/us-west1/operations/820e090b-07bf-3c09-8387-e81f1ff3c8af].\n",
            "\n",
            "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
            "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '369021837527-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '369021837527' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=369021837527'.\n",
            "\u001b[1;33mWARNING:\u001b[0m For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.\n",
            "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
            "Created [https://dataproc.googleapis.com/v1/projects/bda-12345/regions/us-west1/clusters/combine] Cluster placed in zone [us-west1-a].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project bda-12345\n",
        "!gcloud config set compute/region us-west1\n",
        "!gcloud config set compute/zone us-west1-a\n",
        "!gcloud config set dataproc/region us-west1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqIS7jhCM3J2",
        "outputId": "141541a8-3fdc-4b15-f2fe-5253dbdb9992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n",
            "Updated property [compute/zone].\n",
            "Updated property [dataproc/region].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RweXt-mIGezz",
        "outputId": "68983104-403c-43f0-b316-608279917403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting combine_test.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile combine_test.py\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "test = sc.textFile('gs://aedp/expanded_data/expanded_test/part-*')\\\n",
        "          .coalesce(1)\\\n",
        "          .saveAsTextFile('gs://aedp/expanded_data/test')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster combine combine_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQv3A6GxMf1Y",
        "outputId": "c24af69c-3576-4886-cc60-5fb58838a746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job [35a0231619ac4684b6fdcd6f04e18516] submitted.\n",
            "Waiting for job output...\n",
            "23/07/29 20:10:12 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "23/07/29 20:10:12 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "23/07/29 20:10:12 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/07/29 20:10:12 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "23/07/29 20:10:12 INFO org.sparkproject.jetty.util.log: Logging initialized @4465ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "23/07/29 20:10:12 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_372-b07\n",
            "23/07/29 20:10:12 INFO org.sparkproject.jetty.server.Server: Started @4571ms\n",
            "23/07/29 20:10:12 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@29a64e0c{HTTP/1.1, (http/1.1)}{0.0.0.0:43847}\n",
            "23/07/29 20:10:13 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at combine-m/10.138.0.54:8032\n",
            "23/07/29 20:10:14 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at combine-m/10.138.0.54:10200\n",
            "23/07/29 20:10:15 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
            "23/07/29 20:10:15 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "23/07/29 20:10:17 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1690661206186_0001\n",
            "23/07/29 20:10:18 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at combine-m/10.138.0.54:8030\n",
            "23/07/29 20:10:20 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=469; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-west1-369021837527-y7kzku9x/b2cdec5e-27ee-4d47-9336-cf6a99aa38bf/spark-job-history\n",
            "23/07/29 20:10:20 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "23/07/29 20:10:20 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=324; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-west1-369021837527-y7kzku9x/b2cdec5e-27ee-4d47-9336-cf6a99aa38bf/spark-job-history\n",
            "23/07/29 20:10:21 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=399; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-west1-369021837527-y7kzku9x/b2cdec5e-27ee-4d47-9336-cf6a99aa38bf/spark-job-history/application_1690661206186_0001.inprogress\n",
            "23/07/29 20:10:22 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=462; previousMaxLatencyMs=324; operationCount=2; context=gs://aedp/expanded_data/test/_temporary/0\n",
            "23/07/29 20:10:23 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_glob_status. latencyMs=192; previousMaxLatencyMs=0; operationCount=1; context=path=gs://aedp/expanded_data/expanded_test/part-*; pattern=org.apache.hadoop.mapred.FileInputFormat$MultiPathFilter@646d299e\n",
            "23/07/29 20:10:23 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 254\n",
            "23/07/29 20:17:41 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://aedp/expanded_data/test/' directory.\n",
            "23/07/29 20:17:41 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=449; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/test/_temporary\n",
            "23/07/29 20:17:41 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=209; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/test/_SUCCESS\n",
            "23/07/29 20:17:41 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@29a64e0c{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "Job [35a0231619ac4684b6fdcd6f04e18516] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/b2cdec5e-27ee-4d47-9336-cf6a99aa38bf/jobs/35a0231619ac4684b6fdcd6f04e18516/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/b2cdec5e-27ee-4d47-9336-cf6a99aa38bf/jobs/35a0231619ac4684b6fdcd6f04e18516/driveroutput\n",
            "jobUuid: 661ec9c1-a6d6-3ef3-93db-bd4b211f0524\n",
            "placement:\n",
            "  clusterName: combine\n",
            "  clusterUuid: b2cdec5e-27ee-4d47-9336-cf6a99aa38bf\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/b2cdec5e-27ee-4d47-9336-cf6a99aa38bf/jobs/35a0231619ac4684b6fdcd6f04e18516/staging/combine_test.py\n",
            "reference:\n",
            "  jobId: 35a0231619ac4684b6fdcd6f04e18516\n",
            "  projectId: bda-12345\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2023-07-29T20:17:43.692123Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2023-07-29T20:10:06.690971Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2023-07-29T20:10:06.725100Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2023-07-29T20:10:07.044019Z'\n",
            "yarnApplications:\n",
            "- name: combine_test.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://combine-m:8088/proxy/application_1690661206186_0001/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "train = sc.textFile('gs://aedp/expanded_data/expanded_data/part-*')\\\n",
        "          .coalesce(1)\\\n",
        "          .saveAsTextFile('gs://aedp/expanded_data/train')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjmmI_g1G7Xy",
        "outputId": "0b07f773-d7a8-491c-909b-07f46b0a554c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster combine train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbyDX5UiG7oL",
        "outputId": "850d0e29-037b-49db-def4-067650bb30b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job [c2f57f9433d64db3ad4b5caf2ffed84b] submitted.\n",
            "Waiting for job output...\n",
            "23/07/24 01:32:07 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "23/07/24 01:32:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "23/07/24 01:32:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/07/24 01:32:07 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "23/07/24 01:32:08 INFO org.sparkproject.jetty.util.log: Logging initialized @3631ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "23/07/24 01:32:08 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_372-b07\n",
            "23/07/24 01:32:08 INFO org.sparkproject.jetty.server.Server: Started @3761ms\n",
            "23/07/24 01:32:08 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@6a12c15a{HTTP/1.1, (http/1.1)}{0.0.0.0:45481}\n",
            "23/07/24 01:32:08 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at combine-m/10.138.0.37:8032\n",
            "23/07/24 01:32:09 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at combine-m/10.138.0.37:10200\n",
            "23/07/24 01:32:10 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
            "23/07/24 01:32:10 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "23/07/24 01:32:11 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1690160614869_0003\n",
            "23/07/24 01:32:12 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at combine-m/10.138.0.37:8030\n",
            "23/07/24 01:32:14 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=240; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-west1-369021837527-y7kzku9x/f1ec7493-3c9c-484e-b601-274b81a3cf37/spark-job-history\n",
            "23/07/24 01:32:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "23/07/24 01:32:14 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=162; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-west1-369021837527-y7kzku9x/f1ec7493-3c9c-484e-b601-274b81a3cf37/spark-job-history\n",
            "23/07/24 01:32:15 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_glob_status. latencyMs=123; previousMaxLatencyMs=0; operationCount=1; context=path=gs://aedp/expanded_data/expanded_data/part-00001; pattern=org.apache.hadoop.mapred.FileInputFormat$MultiPathFilter@5f35c86e\n",
            "23/07/24 01:32:15 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "23/07/24 01:32:21 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=207; previousMaxLatencyMs=162; operationCount=2; context=gs://aedp/expanded_data/train/_temporary/0\n",
            "23/07/24 01:32:21 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 125\n",
            "23/07/24 01:35:52 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://aedp/expanded_data/train/' directory.\n",
            "23/07/24 01:35:52 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=443; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/train/_temporary\n",
            "23/07/24 01:35:53 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=235; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/train/_SUCCESS\n",
            "23/07/24 01:35:53 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@6a12c15a{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "Job [c2f57f9433d64db3ad4b5caf2ffed84b] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/f1ec7493-3c9c-484e-b601-274b81a3cf37/jobs/c2f57f9433d64db3ad4b5caf2ffed84b/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/f1ec7493-3c9c-484e-b601-274b81a3cf37/jobs/c2f57f9433d64db3ad4b5caf2ffed84b/driveroutput\n",
            "jobUuid: 8cc2d35f-9174-342e-937d-d2ab8dd90d5a\n",
            "placement:\n",
            "  clusterName: combine\n",
            "  clusterUuid: f1ec7493-3c9c-484e-b601-274b81a3cf37\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/f1ec7493-3c9c-484e-b601-274b81a3cf37/jobs/c2f57f9433d64db3ad4b5caf2ffed84b/staging/train.py\n",
            "reference:\n",
            "  jobId: c2f57f9433d64db3ad4b5caf2ffed84b\n",
            "  projectId: bda-12345\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2023-07-24T01:35:57.199439Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2023-07-24T01:32:03.460206Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2023-07-24T01:32:03.498896Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2023-07-24T01:32:03.751783Z'\n",
            "yarnApplications:\n",
            "- name: train.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://combine-m:8088/proxy/application_1690160614869_0003/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters delete split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na1ojSFHn3GN",
        "outputId": "20f6ef38-9a72-4f42-96bd-135d8b52d513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cluster 'split' and all attached disks will be deleted.\n",
            "\n",
            "Do you want to continue (Y/n)?  y\n",
            "\n",
            "Waiting on operation [projects/bda-12345/regions/us-west1/operations/b72fdeb1-65ae-3830-944f-b08dfcd5031f].\n",
            "Deleted [https://dataproc.googleapis.com/v1/projects/bda-12345/regions/us-west1/clusters/split].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vg5WXDN8fB9X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}