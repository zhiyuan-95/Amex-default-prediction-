{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiyuan-95/Amex-default-prediction-/blob/main/accessAndExpand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHS-qRjXWR64"
      },
      "source": [
        "# get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxT-YGI84SG6",
        "outputId": "75ff1f27-25e3-43a9-aa0f-e1ef1e77c4a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "%%shell\n",
        "pip --quiet install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZIKN81g5oHa"
      },
      "source": [
        "### download data from kaggle ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Upload the kaggle.json file\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "1yi6Q_ttgZb-",
        "outputId": "07a2be66-72ac-4e06-b46d-28547333f884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fb282d98-dfd1-429a-9f12-c3ea8cfb6155\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fb282d98-dfd1-429a-9f12-c3ea8cfb6155\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"empanadas\",\"key\":\"27d0db8ad40443c4bc6929322d9c2d3c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNS82bm0e0RX",
        "outputId": "efff6e1b-3231-4c60-b44a-e96811160394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "ZKYpbS1Gfcbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "071YeUgUk5eE"
      },
      "outputs": [],
      "source": [
        "import kaggle\n",
        "import subprocess\n",
        "api_command = 'kaggle competitions download -c amex-default-prediction'\n",
        "subprocess.run(api_command, shell = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5-T2V-p56uW"
      },
      "source": [
        "### unzip to colab##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwU0Ku_c76sD",
        "outputId": "0c2c9b38-543c-4e20-b4b6-f9c9c03aee81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avhl1YJt2o5T"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('amex-default-prediction.zip','r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IJGGI4_6KcR"
      },
      "source": [
        "### upload to google storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu3dS_qkwN7I"
      },
      "outputs": [],
      "source": [
        "import google.colab\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDB3gMMwUjf"
      },
      "outputs": [],
      "source": [
        "google.colab.auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBqgQuoUwEMi"
      },
      "outputs": [],
      "source": [
        "client = storage.Client()\n",
        "bucket_name = 'aedp'\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "file_path = '/content/train_labels.csv'\n",
        "blob = bucket.blob('train_labels.csv')\n",
        "blob.upload_from_filename(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX5_2ratz4hN"
      },
      "source": [
        "# data expansion\n",
        "###1. get numerical part of data and categorical part of data\n",
        "\n",
        "###2. expand the data set\n",
        "\n",
        "*   for the numerical part, using the describe() function in pandas to get the statistic of a customer's bank statement variables\n",
        "*   for the categorical part('D_63', 'D_64', 'B_31'), also use describe in pandas to get statistic\n",
        "\n",
        "###3. combine those two part of information, and also combine all the customers in rdd then we get a new data set\n",
        "\n",
        "###4. those processes are run on the dataproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOnhhULoJ8aT",
        "outputId": "2fd913cb-a08d-4b43-fb6b-7d96ff5915bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=JVLt8Hl0cb0RXybNkhKhjJkazvbWZC&prompt=consent&access_type=offline&code_challenge=1ZsOOJHw1tAX74W6KjjqItLOJhx6fI1eLUvyyHGAKFM&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AZEOvhXDpc6EHeJ-5OnWALfNPgFxzlo3adAexVAeBTNjR_KmnDQq6EyCChufZUbsmbwKcg\n",
            "\n",
            "You are now logged in as [zhiyuan.jin1201@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEyBhY_0ujT",
        "outputId": "02cfcd5e-ccdf-4a53-80b5-cc651408d97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://aedp/original_data/train_data.csv...\n",
            "/ [0 files][    0.0 B/ 15.3 GiB]                                                \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "Resuming download for /content/training_set.csv\n",
            "/ [1 files][ 15.3 GiB/ 15.3 GiB]  101.0 MiB/s                                   \n",
            "Operation completed over 1 objects/15.3 GiB.                                     \n",
            "Copying gs://aedp/original_data/train_labels.csv...\n",
            "/ [1 files][ 29.3 MiB/ 29.3 MiB]                                                \n",
            "Operation completed over 1 objects/29.3 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://aedp/original_data/train_data.csv /content/training_set.csv\n",
        "!gsutil cp gs://aedp/original_data/train_labels.csv /content/label.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y9kI7qkTMOK",
        "outputId": "c22baa6a-4337-4f1d-fdc9-0cfb36013a58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "pip --quiet install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMMH1HYARlAA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RdcBs694oGr"
      },
      "outputs": [],
      "source": [
        "rdd_feature = sc.textFile('training_set.csv')\n",
        "rdd_labels = sc.textFile('label.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5e3x9XLUsZ4"
      },
      "outputs": [],
      "source": [
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUY-d_vfW3m5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N3vKyx7li_l"
      },
      "outputs": [],
      "source": [
        "## get a smaller sample to try on local device\n",
        "sample_rdd = sc.parallelize(rdd_feature.take(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy86HHaH4wXq"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "sample = sample_rdd.mapPartitionsWithIndex(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgVGbeyUuhst"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI1LKhvMo_8X"
      },
      "outputs": [],
      "source": [
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh2_jzJK95Xf"
      },
      "outputs": [],
      "source": [
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE-J0UpsUtW0"
      },
      "outputs": [],
      "source": [
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQv9XA_1gBBV"
      },
      "outputs": [],
      "source": [
        "expansion = sample.mapPartitionsWithIndex(encode_cat)\n",
        "    map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR0lXzG1fkHs"
      },
      "outputs": [],
      "source": [
        "labels = dict(rdd_labels.mapPartitionsWithIndex(split).collect())\n",
        "add_labels = expansion.map(lambda x: x+[labels[x[0]]])\\\n",
        "                      .map(lambda x: (','.join([str(flt) for flt in x])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9NnBP3Mnza-"
      },
      "outputs": [],
      "source": [
        "num_col = []\n",
        "cat_col = []\n",
        "\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "cat_col.append('target')\n",
        "\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = '.'.join(num_col+cat_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dsYKk4HN1ik"
      },
      "outputs": [],
      "source": [
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(add_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv9nIiiTPC8G"
      },
      "outputs": [],
      "source": [
        "!gcloud dataproc clusters create expansion --enable-component-gateway --region us-central1 --zone us-central1-b --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --project bda-12345"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31g8J5WgYej6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6df0d3-1932-4c7c-8a9a-93e66adc49b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n",
            "Updated property [compute/zone].\n",
            "Updated property [dataproc/region].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project bda-12345\n",
        "!gcloud config set compute/region us-central1\n",
        "!gcloud config set compute/zone us-central1-a\n",
        "!gcloud config set dataproc/region us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkASK62bGFTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0def725a-8718-4208-f5c1-515268322cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting expansion.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile expansion.py\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "rdd_feature = sc.textFile('gs://aedp/original_data/train_data.csv')\n",
        "rdd_labels = sc.textFile('gs://aedp/original_data/train_labels.csv')\n",
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "feature = rdd_feature.mapPartitionsWithIndex(split)\n",
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "\n",
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part\n",
        "\n",
        "expansion = feature.map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))\n",
        "labels = dict(rdd_labels.mapPartitionsWithIndex(split).collect())\n",
        "add_labels = expansion.map(lambda x: x+[labels[x[0]]])\\\n",
        "                      .map(lambda x: (','.join([str(flt) for flt in x])))\n",
        "num_col = []\n",
        "cat_col = []\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "cat_col.append('target')\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(add_labels).saveAsTextFile('gs://aedp/expanded_data/expanded_data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_expansion.py\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "rdd_feature = sc.textFile('gs://aedp/original_data/test_data.csv')\n",
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "feature = rdd_feature.mapPartitionsWithIndex(split)\n",
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "\n",
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part\n",
        "\n",
        "expansion = feature.map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))\n",
        "num_col = []\n",
        "cat_col = []\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(expansion).saveAsTextFile('gs://aedp/expanded_data/expanded_test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v223iKne2PfH",
        "outputId": "1c447d59-1489-43bc-b489-9374de10f723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_expansion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IenUn36Yb56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c93924c-ae22-4a60-f71a-ba327fd06861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job [4252a113eede4f1c8302fbb691de7606] submitted.\n",
            "Waiting for job output...\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/07/17 22:09:55 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.util.log: Logging initialized @3727ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_372-b07\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.server.Server: Started @3801ms\n",
            "23/07/17 22:09:55 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@729c8284{HTTP/1.1, (http/1.1)}{0.0.0.0:34503}\n",
            "23/07/17 22:09:56 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at expansion-m/10.128.0.38:8032\n",
            "23/07/17 22:09:56 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at expansion-m/10.128.0.38:10200\n",
            "23/07/17 22:09:57 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
            "23/07/17 22:09:57 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "23/07/17 22:09:58 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1689534591183_0001\n",
            "23/07/17 22:09:59 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at expansion-m/10.128.0.38:8030\n",
            "23/07/17 22:10:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=154; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-369021837527-83jn524y/650cf5e8-c690-41b8-b42b-6f7b268a58fe/spark-job-history\n",
            "23/07/17 22:10:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "23/07/17 22:10:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=176; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-369021837527-83jn524y/650cf5e8-c690-41b8-b42b-6f7b268a58fe/spark-job-history\n",
            "23/07/17 22:10:02 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "23/07/17 22:10:13 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=235; previousMaxLatencyMs=176; operationCount=2; context=gs://aedp/expanded_data/expanded_test/_temporary/0\n",
            "23/07/18 16:33:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://aedp/expanded_data/expanded_test/' directory.\n",
            "23/07/18 16:33:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=363; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/expanded_test/_temporary\n",
            "23/07/18 16:33:01 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=201; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/expanded_test/_SUCCESS\n",
            "23/07/18 16:33:01 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@729c8284{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "Job [4252a113eede4f1c8302fbb691de7606] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-369021837527-o7bhvebh/google-cloud-dataproc-metainfo/650cf5e8-c690-41b8-b42b-6f7b268a58fe/jobs/4252a113eede4f1c8302fbb691de7606/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-369021837527-o7bhvebh/google-cloud-dataproc-metainfo/650cf5e8-c690-41b8-b42b-6f7b268a58fe/jobs/4252a113eede4f1c8302fbb691de7606/driveroutput\n",
            "jobUuid: 2fe5bee0-50f9-31e4-94da-0f432c40a434\n",
            "placement:\n",
            "  clusterName: expansion\n",
            "  clusterUuid: 650cf5e8-c690-41b8-b42b-6f7b268a58fe\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-369021837527-o7bhvebh/google-cloud-dataproc-metainfo/650cf5e8-c690-41b8-b42b-6f7b268a58fe/jobs/4252a113eede4f1c8302fbb691de7606/staging/test_expansion.py\n",
            "reference:\n",
            "  jobId: 4252a113eede4f1c8302fbb691de7606\n",
            "  projectId: bda-12345\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2023-07-18T16:33:05.948191Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2023-07-17T22:09:49.511013Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2023-07-17T22:09:49.559395Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2023-07-17T22:09:49.883040Z'\n",
            "yarnApplications:\n",
            "- name: test_expansion.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://expansion-m:8088/proxy/application_1689534591183_0001/\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster expansion test_expansion.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ObWYKY9OOvf"
      },
      "outputs": [],
      "source": [
        "!gcloud dataproc clusters delete expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbIRc14It7nm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk1+H2l48+eM/ppoY4XOSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}