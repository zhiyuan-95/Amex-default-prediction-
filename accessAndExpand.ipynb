{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiyuan-95/Amex-default-prediction-/blob/main/accessAndExpand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHS-qRjXWR64"
      },
      "source": [
        "# get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxT-YGI84SG6",
        "outputId": "75ff1f27-25e3-43a9-aa0f-e1ef1e77c4a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "pip --quiet install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZIKN81g5oHa"
      },
      "source": [
        "### download data from kaggle ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "1yi6Q_ttgZb-",
        "outputId": "07a2be66-72ac-4e06-b46d-28547333f884"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fb282d98-dfd1-429a-9f12-c3ea8cfb6155\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fb282d98-dfd1-429a-9f12-c3ea8cfb6155\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"empanadas\",\"key\":\"27d0db8ad40443c4bc6929322d9c2d3c\"}'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# Upload the kaggle.json file\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNS82bm0e0RX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKYpbS1Gfcbn"
      },
      "outputs": [],
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "071YeUgUk5eE"
      },
      "outputs": [],
      "source": [
        "import kaggle\n",
        "import subprocess\n",
        "api_command = 'kaggle competitions download -c amex-default-prediction'\n",
        "subprocess.run(api_command, shell = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5-T2V-p56uW"
      },
      "source": [
        "### unzip to colab##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwU0Ku_c76sD",
        "outputId": "0c2c9b38-543c-4e20-b4b6-f9c9c03aee81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avhl1YJt2o5T"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('amex-default-prediction.zip','r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IJGGI4_6KcR"
      },
      "source": [
        "### upload to google storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu3dS_qkwN7I"
      },
      "outputs": [],
      "source": [
        "import google.colab\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwDB3gMMwUjf"
      },
      "outputs": [],
      "source": [
        "google.colab.auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBqgQuoUwEMi"
      },
      "outputs": [],
      "source": [
        "client = storage.Client()\n",
        "bucket_name = 'aedp'\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "file_path = '/content/train_labels.csv'\n",
        "blob = bucket.blob('train_labels.csv')\n",
        "blob.upload_from_filename(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX5_2ratz4hN"
      },
      "source": [
        "# data expansion\n",
        "###1. get numerical part of data and categorical part of data\n",
        "\n",
        "###2. expand the data set\n",
        "\n",
        "*   for the numerical part, using the describe() function in pandas to get the statistic of a customer's bank statement variables\n",
        "*   for the categorical part('D_63', 'D_64', 'B_31'), also use describe in pandas to get statistic\n",
        "\n",
        "###3. combine those two part of information, and also combine all the customers in rdd then we get a new data set\n",
        "\n",
        "###4. those processes are run on the dataproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOnhhULoJ8aT",
        "outputId": "83329a3a-128f-4cb7-d4fa-e322b1f205b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=LNsM9wTGP8xrSwn6CcK4s70ODujhEo&prompt=consent&access_type=offline&code_challenge=KvB6NiiDJS26o06HQ6Ei0eFfwnjPehxzb0D3NImB2G8&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AZEOvhU7_k00WXRDfz67_KJKhMm7YzzzUe8M8eCoIRGAOUdQeDEYzW9OAY-sJu9y0UsVAg\n",
            "\n",
            "You are now logged in as [zhiyuan.jin1201@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEyBhY_0ujT",
        "outputId": "3b907105-7fd0-41a6-bd1f-f3fba69abe21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://aedp/original_data/train_data.csv...\n",
            "/ [0 files][    0.0 B/ 15.3 GiB]                                                \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "\\ [1 files][ 15.3 GiB/ 15.3 GiB]   90.4 MiB/s                                   \n",
            "Operation completed over 1 objects/15.3 GiB.                                     \n",
            "Copying gs://aedp/original_data/train_labels.csv...\n",
            "/ [1 files][ 29.3 MiB/ 29.3 MiB]                                                \n",
            "Operation completed over 1 objects/29.3 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://aedp/original_data/train_data.csv /content/training_set.csv\n",
        "!gsutil cp gs://aedp/original_data/train_labels.csv /content/label.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y9kI7qkTMOK",
        "outputId": "9562cb27-7164-45dc-879d-65283f40e64b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "pip --quiet install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtKMXH0MDIY9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMMH1HYARlAA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RdcBs694oGr"
      },
      "outputs": [],
      "source": [
        "rdd_feature = sc.textFile('training_set.csv')\n",
        "rdd_labels = sc.textFile('label.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5e3x9XLUsZ4"
      },
      "outputs": [],
      "source": [
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUY-d_vfW3m5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N3vKyx7li_l"
      },
      "outputs": [],
      "source": [
        "## get a smaller sample to try on local device\n",
        "sample_rdd = sc.parallelize(rdd_feature.take(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy86HHaH4wXq"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "sample = sample_rdd.mapPartitionsWithIndex(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgVGbeyUuhst"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQv9XA_1gBBV"
      },
      "outputs": [],
      "source": [
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "feature = sample_rdd.mapPartitionsWithIndex(split)\n",
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "\n",
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part\n",
        "\n",
        "expansion = feature.map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))\\\n",
        "                  .map(lambda x: ','.join([str(flt) for flt in x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oehEgSo5EfvD"
      },
      "outputs": [],
      "source": [
        "expansion.saveAsTextFile('testFile')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR0lXzG1fkHs"
      },
      "outputs": [],
      "source": [
        "labels = dict(rdd_labels.mapPartitionsWithIndex(split).collect())\n",
        "add_labels = expansion.map(lambda x: x+[labels[x[0]]])\\\n",
        "                      .map(lambda x: (','.join([str(flt) for flt in x])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9NnBP3Mnza-"
      },
      "outputs": [],
      "source": [
        "num_col = []\n",
        "cat_col = []\n",
        "\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "cat_col.append('target')\n",
        "\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = '.'.join(num_col+cat_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iws65104FIJ1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dsYKk4HN1ik"
      },
      "outputs": [],
      "source": [
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(expansion)\n",
        "output.saveAsTextFile('textFile')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv9nIiiTPC8G",
        "outputId": "f808ce86-2d03-4400-e021-fbf6d284db56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on operation [projects/bda-12345/regions/us-central1/operations/479da88c-7179-30d5-93f6-9d5ce51ef34b].\n",
            "\n",
            "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
            "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '369021837527-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '369021837527' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=369021837527'.\n",
            "\u001b[1;33mWARNING:\u001b[0m For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.\n",
            "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.clusters.create) Operation [projects/bda-12345/regions/us-central1/operations/479da88c-7179-30d5-93f6-9d5ce51ef34b] failed: The zone 'projects/bda-12345/zones/us-central1-c' does not have enough resources available to fulfill the request.  Try a different zone, or try again later..\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters create cluster-7b82 --enable-component-gateway --region us-east1 --zone us-east1-c --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 300 --image-version 2.0-debian10 --project bda-12345"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31g8J5WgYej6",
        "outputId": "843ce3ea-05c8-475c-cbb0-9d310911f2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n",
            "Updated property [compute/zone].\n",
            "Updated property [dataproc/region].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project bda-12345\n",
        "!gcloud config set compute/region us-east1\n",
        "!gcloud config set compute/zone us-east1-c\n",
        "!gcloud config set dataproc/region us-east1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkASK62bGFTB",
        "outputId": "d6a7d673-d00d-4e52-8155-fdf009d69d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting expansion.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile expansion.py\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "rdd_feature = sc.textFile('gs://aedp/original_data/train_data.csv')\n",
        "rdd_labels = sc.textFile('gs://aedp/original_data/train_labels.csv')\n",
        "\n",
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "feature = rdd_feature.mapPartitionsWithIndex(split)\n",
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "\n",
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part\n",
        "\n",
        "expansion = feature.map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))\n",
        "labels = dict(rdd_labels.mapPartitionsWithIndex(split).collect())\n",
        "add_labels = expansion.map(lambda x: x+[labels[x[0]]])\\\n",
        "                      .map(lambda x: (','.join([str(flt) for flt in x])))\n",
        "num_col = []\n",
        "cat_col = []\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "cat_col.append('target')\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(add_labels).saveAsTextFile('gs://aedp/expanded_data/expanded_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v223iKne2PfH",
        "outputId": "60d9873c-ed3c-4226-9289-253f28006d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_expansion.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_expansion.py\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "rdd_feature = sc.textFile('gs://aedp/original_data/test_data.csv')\n",
        "cat= ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "col = rdd_feature.take(1)[0].split(',')\n",
        "num = col[:]\n",
        "for x in cat:\n",
        "  num.remove(x)\n",
        "num.remove('S_2')\n",
        "def split(_, lines):\n",
        "  if _==0:\n",
        "    next(lines)\n",
        "  for r in csv.reader(lines):\n",
        "    yield r\n",
        "feature = rdd_feature.mapPartitionsWithIndex(split)\n",
        "def preprocess_num(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "def strToFloat(df_nu):\n",
        "  df_nu = df_nu.replace('',np.nan).astype(float)\n",
        "  return df_nu\n",
        "\n",
        "def explore(rows):\n",
        "  df = pd.DataFrame(rows, columns = col[1:])\n",
        "  df_cat = df[cat]\n",
        "  df_num = pd.DataFrame(rows, columns = col[1:]).drop(columns =cat+['S_2'])\n",
        "  df_num = strToFloat(df_num)\n",
        "  des_num = df_num.describe()\n",
        "  num_part = list(np.array(des_num).reshape(1416))\n",
        "  des_cat = df_cat.describe()\n",
        "  cat_part = list(np.array(des_cat).reshape(44))\n",
        "  return num_part+cat_part\n",
        "\n",
        "expansion = feature.map(lambda x: (x[0], x[1:]))\\\n",
        "                  .groupByKey()\\\n",
        "                  .mapValues(explore)\\\n",
        "                  .map(lambda x: ([x[0]]+x[1]))\\\n",
        "                  .map(lambda x: ','.join([str(flt) for flt in x]))\n",
        "num_col = []\n",
        "cat_col = []\n",
        "for x in ['count', 'unique', 'top', 'freq']:\n",
        "  for y in cat:\n",
        "    cat_col.append(y+'_'+x)\n",
        "for x in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "  for y in num[1:]:\n",
        "    num_col.append(y+'_'+x)\n",
        "\n",
        "head = sc.parallelize([','.join(['customer_ID']+num_col+cat_col)])\n",
        "output = head.union(expansion)\\\n",
        "             .saveAsTextFile('gs://aedp/expanded_data/expanded_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IenUn36Yb56",
        "outputId": "98826c58-0f5d-4ce9-dc32-dad5ea094dbf"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job [576f5904d0c04b10bf85344bb0416feb] submitted.\n",
            "Waiting for job output...\n",
            "23/07/28 02:08:47 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/07/28 02:08:47 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/07/28 02:08:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/07/28 02:08:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/07/28 02:08:49 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at cluster-7b82-m.us-east1-c.c.bda-12345.internal./10.142.0.10:8032\n",
            "23/07/28 02:08:49 INFO AHSProxy: Connecting to Application History server at cluster-7b82-m.us-east1-c.c.bda-12345.internal./10.142.0.10:10200\n",
            "23/07/28 02:08:50 INFO Configuration: resource-types.xml not found\n",
            "23/07/28 02:08:50 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "23/07/28 02:08:51 INFO YarnClientImpl: Submitted application application_1690509826647_0001\n",
            "23/07/28 02:08:52 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at cluster-7b82-m.us-east1-c.c.bda-12345.internal./10.142.0.10:8030\n",
            "23/07/28 02:08:54 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=223; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-east1-369021837527-jnpyewfx/5f0af98e-4436-467f-8c5a-76f775d71826/spark-job-history\n",
            "23/07/28 02:08:54 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "23/07/28 02:08:54 WARN GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=154; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-east1-369021837527-jnpyewfx/5f0af98e-4436-467f-8c5a-76f775d71826/spark-job-history\n",
            "23/07/28 02:08:55 WARN GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=114; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-east1-369021837527-jnpyewfx/5f0af98e-4436-467f-8c5a-76f775d71826/spark-job-history/application_1690509826647_0001.inprogress\n",
            "23/07/28 02:08:56 INFO FileInputFormat: Total input files to process : 1\n",
            "23/07/28 02:09:12 WARN GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=198; previousMaxLatencyMs=154; operationCount=2; context=gs://aedp/expanded_data/expanded_test/_temporary/0\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster cluster-7b82 test_expansion.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ObWYKY9OOvf"
      },
      "outputs": [],
      "source": [
        "!gcloud dataproc clusters delete expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbIRc14It7nm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdfIkcK/QcJcVDm9wuQN6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}