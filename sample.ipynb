{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZhMykcnAWQYs7GUGKoi/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiyuan-95/Amex-default-prediction-/blob/main/sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8wcZJdOApLq",
        "outputId": "d72d985e-6cac-4cda-b0b8-1a6be8d14f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=0jWnnivkyhqTsLA5Mq6weimwC1juxR&prompt=consent&access_type=offline&code_challenge=LdY27I9ieF-qYu1JBJf_eL-CwpSPjaa8UA7IjMMwlYw&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AZEOvhVm4P7IUnLtHuhU834ox-mK7Lp08ez2hq9N5edUqdzxoDMKO4YKNvcFxniEV1gxbQ\n",
            "\n",
            "You are now logged in as [zhiyuan.jin1201@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project bda-12345\n",
        "!gcloud config set compute/region us-west1\n",
        "!gcloud config set compute/zone us-west1-a\n",
        "!gcloud config set dataproc/region us-west1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbwP8NBelgVN",
        "outputId": "359c3588-dd25-494d-98c9-98bfa043307a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n",
            "Updated property [compute/zone].\n",
            "Updated property [dataproc/region].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters create sample --enable-component-gateway --region us-west1 --zone us-west1-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --project bda-12345"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9hldrl6loTV",
        "outputId": "f009da5f-147d-4ddc-b7f4-4d23aef0aa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on operation [projects/bda-12345/regions/us-west1/operations/af9add4f-4957-3b84-b5be-7dd9eb500eea].\n",
            "\n",
            "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
            "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '369021837527-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '369021837527' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=369021837527'.\n",
            "\u001b[1;33mWARNING:\u001b[0m For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.\n",
            "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
            "Created [https://dataproc.googleapis.com/v1/projects/bda-12345/regions/us-west1/clusters/sample] Cluster placed in zone [us-west1-a].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sample.py\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "\n",
        "header = sc.textFile('gs://aedp/expanded_data/expanded_data/part-00001').take(1)[0]\n",
        "train = sc.textFile('gs://aedp/expanded_data/expanded_data/part-*')\\\n",
        "  .sample(False, 0.05)\\\n",
        "  .coalesce(1)\\\n",
        "  .mapPartitions(lambda x: itertools.chain([header], x))\n",
        "\n",
        "test = sc.textFile('gs://aedp/expanded_data/expanded_data/part-*')\\\n",
        "          .subtract(train)\\\n",
        "          .sample(False, 0.2)\\\n",
        "          .coalesce(1)\\\n",
        "          .mapPartitions(lambda x: itertools.chain([header], x))\n",
        "\n",
        "train.saveAsTextFile('gs://aedp/expanded_data/sample_train')\n",
        "test.saveAsTextFile('gs://aedp/expanded_data/sample_test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxUzByQ8jCSg",
        "outputId": "2af9e5ca-fa3d-4f44-98ce-0d3d1f5d6568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sample.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster sample sample.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9pDy3dRqT2X",
        "outputId": "3e53e2e4-8947-4f01-cd06-162495ec78d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job [837292a1e52040fca80ef4a05630b582] submitted.\n",
            "Waiting for job output...\n",
            "23/07/17 23:05:04 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "23/07/17 23:05:04 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "23/07/17 23:05:04 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/07/17 23:05:04 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "23/07/17 23:05:04 INFO org.sparkproject.jetty.util.log: Logging initialized @3465ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "23/07/17 23:05:04 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_372-b07\n",
            "23/07/17 23:05:04 INFO org.sparkproject.jetty.server.Server: Started @3595ms\n",
            "23/07/17 23:05:04 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@8510ef2{HTTP/1.1, (http/1.1)}{0.0.0.0:39517}\n",
            "23/07/17 23:05:05 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at sample-m/10.138.0.33:8032\n",
            "23/07/17 23:05:05 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at sample-m/10.138.0.33:10200\n",
            "23/07/17 23:05:06 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
            "23/07/17 23:05:06 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "23/07/17 23:05:07 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1689634899086_0002\n",
            "23/07/17 23:05:08 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at sample-m/10.138.0.33:8030\n",
            "23/07/17 23:05:10 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=195; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-west1-369021837527-y7kzku9x/8a135970-c774-4911-9122-07e0d3aff0b1/spark-job-history\n",
            "23/07/17 23:05:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "23/07/17 23:05:10 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=204; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-west1-369021837527-y7kzku9x/8a135970-c774-4911-9122-07e0d3aff0b1/spark-job-history\n",
            "23/07/17 23:05:11 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "23/07/17 23:05:16 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_glob_status. latencyMs=112; previousMaxLatencyMs=90; operationCount=2; context=path=gs://aedp/expanded_data/expanded_data/part-*; pattern=org.apache.hadoop.mapred.FileInputFormat$MultiPathFilter@384d4f90\n",
            "23/07/17 23:05:16 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 125\n",
            "23/07/17 23:05:16 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 125\n",
            "23/07/17 23:08:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://aedp/expanded_data/sample_train/' directory.\n",
            "23/07/17 23:08:05 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=458; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/sample_train/_temporary\n",
            "23/07/17 23:08:05 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=177; previousMaxLatencyMs=0; operationCount=1; context=gs://aedp/expanded_data/sample_train/_SUCCESS\n",
            "23/07/17 23:14:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://aedp/expanded_data/sample_test/' directory.\n",
            "23/07/17 23:14:55 WARN com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=207; previousMaxLatencyMs=177; operationCount=2; context=gs://aedp/expanded_data/sample_test/_SUCCESS\n",
            "23/07/17 23:14:55 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@8510ef2{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "Job [837292a1e52040fca80ef4a05630b582] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/8a135970-c774-4911-9122-07e0d3aff0b1/jobs/837292a1e52040fca80ef4a05630b582/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/8a135970-c774-4911-9122-07e0d3aff0b1/jobs/837292a1e52040fca80ef4a05630b582/driveroutput\n",
            "jobUuid: 0427ebee-de60-3e8f-93e5-eb6004559af8\n",
            "placement:\n",
            "  clusterName: sample\n",
            "  clusterUuid: 8a135970-c774-4911-9122-07e0d3aff0b1\n",
            "pysparkJob:\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/8a135970-c774-4911-9122-07e0d3aff0b1/jobs/837292a1e52040fca80ef4a05630b582/staging/sample.py\n",
            "reference:\n",
            "  jobId: 837292a1e52040fca80ef4a05630b582\n",
            "  projectId: bda-12345\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2023-07-17T23:14:58.614349Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2023-07-17T23:04:59.480367Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2023-07-17T23:04:59.518267Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2023-07-17T23:04:59.703812Z'\n",
            "yarnApplications:\n",
            "- name: sample.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://sample-m:8088/proxy/application_1689634899086_0002/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters delete sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyNZR0u_Vv5r",
        "outputId": "12b2ed76-df34-44c3-f414-9e8a85e0a851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cluster 'sample' and all attached disks will be deleted.\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "Waiting on operation [projects/bda-12345/regions/us-west1/operations/95d9c955-ea9b-3470-92ea-7e3b7ce2a063].\n",
            "Deleted [https://dataproc.googleapis.com/v1/projects/bda-12345/regions/us-west1/clusters/sample].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile split_test.py\n",
        "import itertools\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()\n",
        "\n",
        "header = sc.textFile('gs://aedp/expanded_data/expanded_data/part-00001').take(1)[0]\n",
        "train = sc.textFile('gs://aedp/expanded_data/expanded_data/part-*')\\\n",
        "  .sample(False, 0.5)\\\n",
        "  .coalesce(1)\\\n",
        "  .mapPartitions(lambda x: itertools.chain([header], x))\n",
        "\n",
        "test = sc.textFile('gs://aedp/expanded_data/expanded_data/part-*')\\\n",
        "          .subtract(train)\\\n",
        "          .sample(False, 0.5)\\\n",
        "          .coalesce(1)\\\n",
        "          .mapPartitions(lambda x: itertools.chain([header], x))\n",
        "\n",
        "train.saveAsTextFile('gs://aedp/expanded_data/sample_train')\n",
        "test.saveAsTextFile('gs://aedp/expanded_data/sample_test')"
      ],
      "metadata": {
        "id": "yMZUN6RfLEUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}